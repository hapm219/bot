from fastapi import FastAPI
from fastapi.middleware.cors import CORSMiddleware
from pydantic import BaseModel
from pathlib import Path
import glob, json, gc
import numpy as np
import faiss
from sentence_transformers import SentenceTransformer
from load_model import load_llm

# ====== C·∫§U H√åNH ======
DATA_DIR = Path("data")
INDEX_DIR = DATA_DIR / "index"
EMBEDDING_PATH = DATA_DIR / "encoder" / "bge-m3"
MAX_TOKENS = 192
TOP_K = 3

# ====== KH·ªûI T·∫†O FASTAPI ======
app = FastAPI()
app.add_middleware(
    CORSMiddleware,
    allow_origins=["*"], allow_methods=["*"], allow_headers=["*"]
)

class ChatRequest(BaseModel):
    question: str

# ====== LOAD M√î H√åNH & INDEX ======
print("üì• ƒêang t·∫£i m√¥ h√¨nh embedding...")
embed_model = SentenceTransformer(str(EMBEDDING_PATH))

print("üì¶ ƒêang t·∫£i to√†n b·ªô FAISS index...")
all_indexes, all_mappings = [], []
for faiss_path in glob.glob(str(INDEX_DIR / "**/index.faiss"), recursive=True):
    try:
        index = faiss.read_index(faiss_path)
        mapping_path = Path(faiss_path).parent / "mapping.json"
        if not mapping_path.exists():
            continue
        with open(mapping_path, encoding="utf-8") as f:
            mapping = json.load(f)
        all_indexes.append(index)
        all_mappings.append(mapping)
        print(f"‚úÖ Loaded index: {faiss_path}")
    except Exception as e:
        print(f"‚ùå L·ªói khi load {faiss_path}: {e}")

print("ü§ñ ƒêang chu·∫©n b·ªã m√¥ h√¨nh LLM...")
llm = load_llm()

# ====== H√ÄM X·ª¨ L√ù ======
def search_similar_chunks(query: str, top_k=TOP_K):
    query_emb = embed_model.encode(
        f"Represent this sentence for searching relevant passages: {query}",
        convert_to_numpy=True
    )
    results = []
    for index, texts in zip(all_indexes, all_mappings):
        D, I = index.search(np.array([query_emb]), top_k)
        results += [(score, texts[str(idx)]) for score, idx in zip(D[0], I[0]) if str(idx) in texts]
    return [text for _, text in sorted(results, key=lambda x: x[0])[:top_k]]

def limit_context(chunks, max_chars=800):
    context = ""
    for c in chunks:
        if len(context) + len(c) > max_chars:
            break
        context += c + "\n\n"
    return context.strip()

# ====== ENDPOINT ======
@app.post("/chat")
async def chat(req: ChatRequest):
    q = req.question.strip()
    chunks = search_similar_chunks(q)
    context = limit_context(chunks)

    prompt = f"""B·∫°n l√† tr·ª£ l√Ω AI IDCee. Tr·∫£ l·ªùi ng·∫Øn g·ªçn, b·∫±ng ti·∫øng Vi·ªát v√† ch·ªâ s·ª≠ d·ª•ng th√¥ng tin t·ª´ ph·∫ßn Th√¥ng tin n·ªôi b·ªô.

üëâ N·∫øu kh√¥ng c√≥ th√¥ng tin ph√π h·ª£p, tr·∫£ l·ªùi duy nh·∫•t c√¢u sau:
"T√¥i kh√¥ng t√¨m th·∫•y th√¥ng tin trong t√†i li·ªáu n·ªôi b·ªô ƒë·ªÉ tr·∫£ l·ªùi c√¢u h·ªèi n√†y."

‚ùó Kh√¥ng ƒë∆∞·ª£c b·ªãa, suy ƒëo√°n ho·∫∑c th√™m n·ªôi dung ngo√†i d·ªØ li·ªáu cung c·∫•p. Ch·ªâ s·ª≠ d·ª•ng n·ªôi dung ch·ª©a t·ª´ kh√≥a: "{q}"

### C√¢u h·ªèi:
{q}

### Th√¥ng tin n·ªôi b·ªô:
{context}

### Tr·∫£ l·ªùi:
"""

    try:
        raw = llm(prompt, max_tokens=MAX_TOKENS)
        if isinstance(raw, str):
            return {"answer": raw.strip()}
        elif hasattr(raw, "__iter__"):
            return {"answer": "".join(chunk for chunk in raw).strip()}
        elif isinstance(raw, dict) and "choices" in raw:
            return {"answer": raw["choices"][0]["text"].strip()}
        else:
            return {"answer": "(Kh√¥ng th·ªÉ x·ª≠ l√Ω ph·∫£n h·ªìi)"}
    except Exception as e:
        return {"answer": f"‚ùå L·ªói infer: {e}"}
